# üß† Embedding Analysis Report
**Date:** November 26, 2025  
**Total Items:** 27,066  
**Items with Embeddings:** 0 ‚ùå

---

## Executive Summary

Your scrapers are extracting metadata using CLIP visual analysis, but **NOT saving the embedding vectors** to the database. This means you need to run the embedding generation process after scraping completes.

## Current Architecture

### ‚úÖ What's Working Well

1. **CLIP Model:** Using `clip-ViT-B-32` via sentence-transformers
   - 512-dimensional vectors (padded to 768)
   - Fast inference (~200ms CPU, ~50ms GPU)
   - Good quality for fashion items

2. **Hybrid Metadata Extraction:**
   - Text extraction (title/description parsing)
   - Visual extraction (CLIP zero-shot classification)
   - Smart merging: prefers visual for color, text for brand
   - Code: `hybrid_metadata_extractor.py`

3. **Multimodal Embedding Support:**
   - `embed_image(image_bytes, text=title)` combines both
   - Better search quality than image-only
   - Already implemented in `backend/embeddings.py`

### ‚ùå What's Missing

1. **No embeddings saved during scraping:**
   ```python
   # Scrapers INSERT without embedding field
   INSERT INTO fashion_items (
       source, external_id, title, description, price, 
       url, image_url,
       brand, category, color, condition, size
   )
   # Missing: embedding field!
   ```

2. **Sequential processing:**
   - `embed_items.py` processes items one-by-one
   - No GPU batching (could be 10x faster)
   - Will take ~2-3 hours for 27K items

## Embedding Generation Process

### Current Setup: `embed_items.py`

**What it does:**
```python
# 1. Downloads image from URL
image_bytes = _download_image(image_url)

# 2. Combines title + description
text_content = f"{title}. {description}".strip()

# 3. Generates MULTIMODAL embedding (image + text)
vector = embed_image(image_bytes, text=text_content)

# 4. Saves to database
UPDATE fashion_items SET embedding = %s WHERE id = %s
```

**Performance:**
- **Current:** ~1-2 items/second (sequential)
- **With GPU batching:** ~20-50 items/second
- **Estimated time for 27K items:** 2-3 hours (CPU) or 20-30 minutes (GPU batch)

**Quality:**
- ‚úÖ Multimodal (image + text) - BEST approach
- ‚úÖ Normalized vectors for cosine similarity
- ‚úÖ Zero-padding to 768 dimensions

## Configuration

```python
# backend/config.py
EMBEDDING_PROVIDER = "clip"  # Using CLIP (not OpenAI)
EMBEDDING_DIMENSION = 768    # Target dimension
OPENAI_API_KEY = None        # Not using OpenAI

# Model details
Model: clip-ViT-B-32
Native dimension: 512
Padding: Zero-pad to 768
Normalization: L2 normalized
```

## Database Schema

```sql
-- Embedding column setup
embedding vector(768)  -- pgvector type

-- HNSW index for fast search
CREATE INDEX fashion_items_embedding_idx 
ON fashion_items 
USING hnsw (embedding vector_cosine_ops) 
WITH (m='16', ef_construction='64');
```

**Index Performance:**
- Query time: 10-50ms with HNSW
- Index size: 172 MB (already built)
- Supports sub-100ms searches on 50M+ items

## Recommendations

### üî• IMMEDIATE ACTION (After scraping finishes)

```bash
cd /Users/harveyhoulahan/Desktop/MiniApp/find-this-fit/ingestion
python3 embed_items.py all
```

This will:
- Process all 27,066 items
- Generate multimodal embeddings (image + text)
- Update database with vectors
- Take 2-3 hours on CPU

### üöÄ OPTIMIZATION #1: Batch Processing

Modify `embed_items.py` to process in batches:

```python
# Instead of:
for item in items:
    vector = embed_image(image_bytes, text)
    save_to_db(vector)

# Use batching:
batch_size = 32
for batch in chunks(items, batch_size):
    images = [download_image(item['url']) for item in batch]
    texts = [f"{item['title']}. {item['description']}" for item in batch]
    
    # Batch encode (10x faster)
    vectors = model.encode(
        list(zip(images, texts)), 
        batch_size=batch_size,
        normalize_embeddings=True
    )
    
    # Batch save to DB
    save_batch_to_db(batch, vectors)
```

**Expected improvement:** 2-3 hours ‚Üí 20-30 minutes

### üéØ OPTIMIZATION #2: Save Embeddings During Scraping

For future scraping runs, modify scrapers to save embeddings:

```python
# In depop_scraper_working.py, grailed_scraper.py, vinted_scraper.py

# After getting item with metadata:
item = enhance_item_metadata_hybrid(item, use_visual=True)

# Generate embedding
from embeddings import embed_image
text = f"{item['title']}. {item.get('description', '')}".strip()
item['embedding'] = embed_image(
    image_bytes,  # Already downloaded for metadata
    text=text
)

# Save to DB with embedding
INSERT INTO fashion_items (
    source, external_id, title, ..., embedding
) VALUES (%s, %s, %s, ..., %s)
```

**Benefit:** No need to re-download images later, saves hours

### ‚ö° OPTIMIZATION #3: Upgrade CLIP Model (Optional)

Current: `clip-ViT-B-32` (512-dim, padded to 768)
Upgrade: `clip-ViT-L-14` (768-dim native, better quality)

```python
# Change in backend/embeddings.py
from sentence_transformers import SentenceTransformer
_clip_model = SentenceTransformer("clip-ViT-L-14")
```

**Trade-off:**
- 15-20% better search quality
- 2x slower inference
- 800MB model vs 350MB

## Search Performance After Embedding

Once embeddings are generated, search will be:

**Image Search:**
```python
# Upload photo ‚Üí get embedding ‚Üí search
vector = embed_image(uploaded_image_bytes)
results = search_similar(vector, limit=20)
# Query time: 10-50ms
```

**Filtered Search:**
```python
# Search + filter by category, brand, price, etc.
results = search_similar_with_filters(
    vector,
    category='jacket',
    brand='Nike',
    color='black',
    min_price=50,
    max_price=200
)
# Query time: 20-100ms
```

## Testing After Embedding

```bash
# Test search
cd ingestion
python3 test_search.py

# Expected output:
# Items with embeddings: 27066
# Text search "vintage nike jacket": [results]
# Image search: [results]
```

## Summary

| Aspect | Current State | After Embedding | Optimized |
|--------|---------------|-----------------|-----------|
| Items | 27,066 | 27,066 | 27,066 |
| Embeddings | 0 ‚ùå | 27,066 ‚úÖ | 27,066 ‚úÖ |
| Processing time | N/A | 2-3 hours | 20-30 min |
| Search ready | No | Yes | Yes |
| Search speed | N/A | 10-50ms | 10-50ms |
| Quality | N/A | Good | Better |

## Next Steps

1. ‚úÖ **Wait for scraping to finish** (Vinted in progress)
2. üî• **Run:** `python3 embed_items.py all` (2-3 hours)
3. ‚úÖ **Run optimization SQL:** `docker exec -i findthisfit-db psql -U postgres -d find_this_fit < database/optimize_indexes.sql`
4. üß™ **Test search:** `python3 test_search.py`
5. üöÄ **Deploy app** with visual search enabled

## Files to Review

- `ingestion/embed_items.py` - Main embedding generator
- `backend/embeddings.py` - CLIP model & multimodal embedding
- `ingestion/hybrid_metadata_extractor.py` - Visual metadata extraction
- `backend/search_enhanced.py` - Enhanced search with filters
- `database/optimize_indexes.sql` - Performance optimization

---

**Key Insight:** Your embeddings are high quality (multimodal CLIP), the process is just not automated during scraping yet. Running `embed_items.py all` after scraping will get you fully operational.
